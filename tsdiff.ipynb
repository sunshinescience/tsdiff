{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the difference between curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of project: \n",
    "-  Find all of the different ways to compare curves. \n",
    "-  Quantify the similarity or dis-similarity of two (and more) curves.\n",
    "-  Perhaps apply ML techniques to classify the (shapes? or numerical values?) curves???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms? Sets of rectangles (similar to bins in a probability distribution plot) between the curves might give sets of values, think about this a little more and see if there is something out there on this???\n",
    "Integration?\n",
    "Some sort of spline interpolation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolmogorov-Smirnov test\n",
    "\n",
    "calculate the cumulative distributions, then calculate the maximum distance between those. \n",
    "This is a relatively robust measure of (dis-)similarity between two distributions. \n",
    "The smaller the dissimilarity, the more the curves are \"alike\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic time warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "piecewise pointwise comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fr√©chet distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: https://www.researchgate.net/post/How_can_I_compare_the_shape_of_two_curves\n",
    "\n",
    "Compare the number of knots, \n",
    "and inflection points.  Compare the interpolation methods for estimating the curves.  \n",
    "Compare the area under the curve (AUC). Compare the powers of the series needed to replicate the curves.  \n",
    "Compare your root mean square error and your mean absolute percentage errors in reproducing the curves. \n",
    "Moreover, compare the number of iterations\n",
    "and the time consumed of a Newton Raphson algorithm needed to estimate the curves. \n",
    "\n",
    "Obviously, for  piecewise smooth lines the inflaction points is crucial. Also the poits where the curves possess edges (say,  knots of weak type?), where the one-sided tangent lines do not coincide. Still open question:\n",
    "How does Liselore understand the notion of shape?\n",
    "Accordingly, one can build the formal definition OR one should open a Permanent Book of Definitions of the Shape of Curves,  with account of the Borsuk Theory etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate curvature as a function of arc length for each curve and see how they compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Compute the Fourier transform of both functions.\n",
    "2 - Compare the absolute values of the transforms using a distance like Itakura or Itakura-Saito. There are others.\n",
    "There is another interesting way that was used in speech recognition based on the \n",
    "Bellman mathematical programing (if my memory does not fail) . \n",
    "Essentially consisted on putting  the values of the two functions in two axis. \n",
    "Then the Cartesian product was done. To go from one point to another there is a cost (may be the distance). \n",
    "The total minimum   cost way gives a distance between the two curves and states the modification to go from one to the other. \n",
    "If they are equal, aside a translation, the way will be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You can calculate the distance between the curves for each x value, evaluate in statistical mode the result. 2 diagram similar will have low sigma\n",
    "2) calculate the spectre of 2 curves (FFT) two similar diagrams will have similar frequency response (module). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of the complexity of the scheme depends on what you are trying to capture between the two curves. However, from your latest post it seems the curves are similar to each other and might be separated by some small shift and scaling. If that is indeed the case, the correlation between the two curves would seem a natural distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like the idea of comparing curvature as a function of arc length for the different curves having the same total arc length. This function is easily constructed for any smooth curve described by parametric equations. But this is only useful if \"similar shapes\" is defined so that similarity is not affected by rotation or displacement. For example, two straight line segments of the same length have exactly the same shape regardless of location or rotational orientation. A quantitative measure of dis-similarities between two curve shapes is any quantitative measure of the error between the two functions of curvature as a function of arc length. Possible examples are the max-norm error, or a root-mean-square error, etc. The selection of this latter error measure would be part of the definition of the measure of dis-similarities between two curve shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose two of the curves are f1(x), f2(x). Consider minimizing the L1 or L2 norm of f1(x+h) - f2(x) with respect to h. This could be done numerically by trying a range of h's. I am assuming that you are only interested in *translating the curves horizontally*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an overlay plot of the two curves, with respect to a common axis.  If that axis is time,  you may want to model the two series and compare their estimation and forecastability either in-sample or post-sample.\n",
    "If the curves are time series, examine them for covariance stationarity prior to any modeling.  \n",
    "Examine the distributions with respect to their moments (mean, variance, skewness, kurtosis), linearity, and tail characteristics.  Determine the number of parameters needed to define those distributions, and whether their moments are clearly defined and computable.   \n",
    "If you cannot use parametric measures to estimate the curves, you may be able to do so with regression splines, in which case, they can be compared according to the number of knots and the type of splines used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like the suggested by tests via some functionals. Why should one check  the best fit if there is no chance seen sometimes 'from the first glance'? One remark: the choice of the functionals requires having them shape invariant. With respect to this, the idea of starting with the piecewise linear functions suggested by  Biswanath and Robert's idea of using splines (see link above) of a determined kind (say, degree and smoothness) with the same number of knots seems to be very appropriate for comparing functions of a single variable in a bounded interval. I think this way can be extended to multidimensional domain (say, surfaces), and even to closed curves through some Bezier approximation.  The value of Robert's note is that one should start with rough estimates for getting less expensive chance of discovering the lack of similarity.\n",
    "Obviously, this leads to problem somehow inverse with respect to those from the image compression - how far can we simplify the starting analysis to be sure that such and such differences will be discovered by the algorithm (the errors of the first and the second kind from statistics are always on our road!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should the end goal be to classify the shapes of the curves? As well finding all of the ways to see the difference between the curves. Can a library for this be made? Apply ML to this???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-ecbaa7b2ebe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing sin and cos\n",
    "x = np.linspace(0, 2 * np.pi)\n",
    "sin = np.sin(x)\n",
    "cos = np.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, sin, label=\"sin\")\n",
    "ax.plot(x, cos, label=\"cos\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffing areas (integrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing sin and cos integrals\n",
    "sin_int = integrate.trapz(sin, x)\n",
    "cos_int = integrate.trapz(cos, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_int - cos_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffing lines (stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing ~flat yield curves\n",
    "\n",
    "# Read in the data into a dataframe using the pandas read_csv function\n",
    "df = pd.read_csv(\"data/yieldcurve.txt\", sep=\"\\t\", na_values=\"N/A\", parse_dates=['Date'])\n",
    "for column in [\"1 mo\",\"2 mo\",\"3 mo\",\"6 mo\",\"1 yr\",\"2 yr\",\"3 yr\",\"5 yr\",\"7 yr\",\"10 yr\",\"20 yr\",\"30 yr\"]:\n",
    "    df[column] = pd.to_numeric(df[column], errors=\"coerce\") # Converting column values to a numeric type, error parameter is set to ‚Äòcoerce‚Äô, so invalid parsing will be set as NaN\n",
    "df = df.set_index(\"Date\") # Set the DataFrame index using the date column\n",
    "\n",
    "# Quick exploration of the data\n",
    "print (df.head(5)) # Examine the contents of the dataframe using the head command, which grabs the first five rows\n",
    "# print (df.tail(5)) # Examine the last five rows\n",
    "\n",
    "# Creating an array for the x-axis, which is maturity \n",
    "maturity = np.array([1/12, 2/12, 3/12, 6/12, 1, 2, 3, 5,  7, 10, 20, 30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute statistics for two sets of measurements:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "slope : slope of the regression line\n",
    "\n",
    "intercept : intercept of the regression line\n",
    "\n",
    "r-value : correlation coefficient\n",
    "\n",
    "p-value : two-sided p-value for a hypothesis test whose null hypothesis is that the slope is zero\n",
    "\n",
    "stderr : Standard error of the estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might work to express their (dis)similarity as a numerical value:\n",
    "for the case of two curves supposedly shifted in x-coordinate\n",
    "Assign parameter t=0 to xmin(first curve) and to xmin(second curve).  Assign parameter t=1 to xmax(second curve) and to xmax(second curve).  Now travel along the first curve (increase t) and note y-values of both curves taken at the same value of parameter t.  No problem if both curves are known only on discrete sets of x's, even if both x-sets do not coincide - just interpolate the y-value of the second curve (or use second curve's point nearest in t).  Collect all pairs (yj(first), yj(second)).  When sampling points for both curves do not coincide (in t), repeat the travel, this time along the second curve.  This way both curves will be treated on equal footing, as they should. Now look at collected pairs (yj(first), yj(second)) and think of them as of pairs of two random variables and compute their average values, standard deviations and finally the coefficient of correlation r.  The closer is the absolute value of correlation coefficient r to unity the more similar are the curves.  Loosely speaking:  |r| > 0.99 - curves are virtually identical, |r| > 0.95 - very high similarity, 0.67 < |r| < 0.95 - high similarity, 0.33 < |r| < 0.67 - some (moderate) similarity, 0.05 < |r| < 0.33 - not very similar, |r| < 0.05 - no similarity at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_ave = sin.mean()\n",
    "cos_ave = cos.mean()\n",
    "\n",
    "sin_stdev = statistics.stdev(sin)\n",
    "cos_stdev = statistics.stdev(cos)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(sin, cos)\n",
    "\n",
    "\"\"\"\n",
    "The closer that the absolute value of r is to one, the better that the data are described by a linear equation. \n",
    "If r =1 or r = -1 then the data set is perfectly aligned. Data sets with values of r close to zero show little \n",
    "to no straight-line relationship.\n",
    "\"\"\"\n",
    "print (r_value) # As expected, these curves are not linear, and thus show r values close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
